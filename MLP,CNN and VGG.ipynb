{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIW84oiM5ZMx",
        "outputId": "5251e0ef-7d16-43b9-832e-51c8224cc719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Loss: 2.1049, Accuracy: 34.87%\n",
            "Epoch [2/10], Loss: 2.0661, Accuracy: 39.12%\n",
            "Epoch [3/10], Loss: 2.0502, Accuracy: 40.77%\n",
            "Epoch [4/10], Loss: 2.0468, Accuracy: 41.17%\n",
            "Epoch [5/10], Loss: 2.0470, Accuracy: 41.18%\n",
            "Epoch [6/10], Loss: 2.0377, Accuracy: 42.17%\n",
            "Epoch [7/10], Loss: 2.0356, Accuracy: 42.36%\n",
            "Epoch [8/10], Loss: 2.0373, Accuracy: 42.27%\n",
            "Epoch [9/10], Loss: 2.0376, Accuracy: 42.27%\n",
            "Epoch [10/10], Loss: 2.0368, Accuracy: 42.30%\n",
            "Test Accuracy: 42.45%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Step 1: Load and preprocess the CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Step 2: Define the architecture of the MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Step 3: Instantiate the model\n",
        "input_size = 32 * 32 * 3\n",
        "hidden_size1 = 512\n",
        "hidden_size2 = 256\n",
        "num_classes = 10\n",
        "\n",
        "model = MLP(input_size, hidden_size1, hidden_size2, num_classes)\n",
        "\n",
        "# Step 4: Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 5: Train the MLP model\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {(correct/total)*100:.2f}%')\n",
        "\n",
        "# Step 6: Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {(correct/total)*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR5t0cWF5adw",
        "outputId": "9ebf2cce-c500-486e-baa2-dbc795195c4d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 40258306.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Loss: 1.4406, Accuracy: 48.18%\n",
            "Epoch [2/10], Loss: 1.0535, Accuracy: 62.66%\n",
            "Epoch [3/10], Loss: 0.8954, Accuracy: 68.48%\n",
            "Epoch [4/10], Loss: 0.7915, Accuracy: 72.22%\n",
            "Epoch [5/10], Loss: 0.7028, Accuracy: 75.35%\n",
            "Epoch [6/10], Loss: 0.6302, Accuracy: 77.89%\n",
            "Epoch [7/10], Loss: 0.5570, Accuracy: 80.44%\n",
            "Epoch [8/10], Loss: 0.4873, Accuracy: 82.91%\n",
            "Epoch [9/10], Loss: 0.4096, Accuracy: 85.85%\n",
            "Epoch [10/10], Loss: 0.3495, Accuracy: 87.79%\n",
            "Test Accuracy: 72.30%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Step 1: Load and preprocess the CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Step 2: Define the architecture of the CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Step 3: Instantiate the model\n",
        "model = CNN()\n",
        "\n",
        "# Step 4: Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 5: Train the CNN model\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {(correct/total)*100:.2f}%')\n",
        "\n",
        "# Step 6: Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {(correct/total)*100:.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSslF-FzCRI8",
        "outputId": "d8bd92ad-54e4-413e-b656-ac64bb06c1e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:03<00:00, 42751676.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 83.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Epoch [1/5], Loss: 2.3413, Accuracy: 39.64%\n",
            "Training - Epoch [2/5], Loss: 1.5816, Accuracy: 55.09%\n",
            "Training - Epoch [3/5], Loss: 1.3380, Accuracy: 61.26%\n",
            "Training - Epoch [4/5], Loss: 1.1555, Accuracy: 66.03%\n",
            "Training - Epoch [5/5], Loss: 1.0014, Accuracy: 70.14%\n",
            "Test Accuracy: 77.73%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Step 1: Load CIFAR-10 or CIFAR-100 dataset and define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to VGG input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Choose between CIFAR-10 and CIFAR-100\n",
        "# CIFAR_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "CIFAR_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_size = int(0.8 * len(CIFAR_dataset))\n",
        "val_size = len(CIFAR_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(CIFAR_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for training, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(CIFAR_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Step 2: Load pre-trained VGG model\n",
        "vgg = models.vgg16(pretrained=True).to(device)\n",
        "\n",
        "# Step 3: Replace the final classification layer with new layer(s)\n",
        "# For CIFAR-10 or CIFAR-100, we need to replace the final fully connected layer\n",
        "num_features = vgg.classifier[6].in_features\n",
        "vgg.classifier[6] = nn.Linear(num_features, len(CIFAR_dataset.classes)).to(device)\n",
        "\n",
        "# Step 4: Freeze the weights of pre-trained layers\n",
        "for param in vgg.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Step 5: Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(vgg.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Step 6: Train the modified VGG model\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vgg.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vgg(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {(correct/total)*100:.2f}%')\n",
        "\n",
        "# Step 7: Evaluate the model on the test set\n",
        "vgg.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = vgg(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {(correct/total)*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The differences in performance among the MLP, CNN, and VGG-based models can be attributed to several factors:\n",
        "\n",
        "Model Complexity: CNNs and VGG-based models are inherently more complex than MLPs. CNNs are specifically designed to handle image data and leverage the spatial structure of images for feature extraction. VGG, being a deeper CNN architecture, has more capacity to learn intricate patterns and features in images compared to MLPs. This increased model complexity allows CNNs and VGG to capture more nuanced information from images, leading to better performance.\n",
        "\n",
        "Feature Extraction: CNNs, including the VGG architecture, utilize convolutional layers that are capable of capturing spatial hierarchies of features in images. These layers apply filters across small regions of the input image, enabling the model to detect local patterns such as edges, textures, and shapes. As the network progresses through the layers, it learns to extract increasingly complex and abstract features. In contrast, MLPs treat images as flattened vectors, thereby disregarding the spatial relationships between pixels. This limitation makes it challenging for MLPs to effectively capture the rich structure present in images.\n",
        "\n",
        "Parameter Sharing and Pooling: CNNs benefit from parameter sharing and pooling operations, which contribute to their ability to generalize well to new data. Parameter sharing refers to the sharing of weights across different regions of the input image, which helps reduce the number of parameters in the model and makes it more robust to variations in the input. Pooling operations (e.g., max pooling) further downsample feature maps, retaining the most relevant information while reducing computational complexity. These operations are absent in MLPs, making them more prone to overfitting and less efficient in handling image data.\n",
        "\n",
        "Transfer Learning (for VGG): VGG is a pre-trained CNN architecture that has been trained on large-scale image datasets such as ImageNet. Transfer learning involves leveraging the knowledge gained by a model trained on one task (e.g., ImageNet classification) and applying it to a related task (e.g., image classification on a different dataset). By using pre-trained weights from VGG as initialization, the model starts with learned features that are likely to be relevant for the new dataset. This initialization helps speed up convergence during training and can lead to better performance, especially when the new dataset is small or similar to the original dataset used for pre-training.\n",
        "\n",
        "In summary, CNNs and VGG-based models outperform MLPs in image classification tasks due to their ability to capture spatial relationships, extract hierarchical features, and leverage parameter sharing and pooling operations. Transfer learning, in the case of VGG, further enhances performance by leveraging pre-trained representations learned from large-scale datasets."
      ],
      "metadata": {
        "id": "WvvS6sBD9Zi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFwRoAAcJ3E0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}